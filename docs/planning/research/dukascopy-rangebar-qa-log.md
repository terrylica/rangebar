# Dukascopy Range Bar Construction - Design Q&A Log
**Status:** ✅ COMPLETE - All Questions Resolved (Q1-Q18)
**Date:** 2025-09-30
**Purpose:** Record design decisions for polishing dukascopy-rangebar-construction.md

---

## Decision Log

### Q1: Volume Strategy (Critical Path) ✅ DECIDED

**Question:** Which volume handling approach should be canonical?

**Options Presented:**
- A) Simple liquidity sum for all instruments
- B) Tick count as volume for all instruments
- C) ⭐ Dual tracking (both metrics available)
- D) Instrument-specific (no dual tracking)

**Decision:** **C - Dual tracking with maximum Forex information retention**

**Implications:**
```rust
struct DukascopyRangeBar {
    pub base: RangeBar,
    pub spread_stats: SpreadStats {
        tick_count: u32,              // Always track
        total_bid_liquidity: FixedPoint,  // Always track
        total_ask_liquidity: FixedPoint,  // Always track
        avg_spread: FixedPoint,
        min_spread: FixedPoint,
        max_spread: FixedPoint,
    }
}
```

**Rationale:**
- Preserves both semantics (liquidity + activity)
- Downstream consumers can choose appropriate metric
- Maximizes information retention for Forex analysis

---

### Q2: Direction Inference Logic ✅ DECIDED

**Question:** How to handle `is_buyer_maker` field for synthetic trades?

**Research Findings:**
- Binance semantics: `is_buyer_maker=true` → buyer is passive (maker), seller aggressive (taker)
- Dukascopy provides **quote data only** (bid/ask/volumes), NOT trade data
- No actual trade direction available for ANY instrument (Forex, Crypto, all)

**Options Considered:**
- A) Infer from bid/ask volume asymmetry: `is_buyer_maker = bid_volume > ask_volume`
- B) Inverted logic
- C) Spread asymmetry heuristic
- D) ⭐ Explicit "unknown" - set to `None`

**Decision:** **D - Set `is_buyer_maker = None` for all Dukascopy data**

**Implications:**
```rust
AggTrade {
    // ... other fields
    is_buyer_maker: None,  // Explicit: direction unknown (quote data, not trades)
    is_best_match: None,   // N/A for Dukascopy
}
```

**Rationale:**
- Honest semantics: Dukascopy = quotes, not trades
- Forces conscious handling by downstream consumers
- Avoids false confidence in inferred/synthetic direction
- `Option<bool>` pattern already exists in struct

---

### Q3: RangeBar Structure Extension ✅ DECIDED

**Question:** How to return range bars with Dukascopy-specific information?

**Options Presented:**
- A) Return standard `RangeBar` (discard spread data)
- B) ⭐ Return wrapped structure `DukascopyRangeBar`
- C) Add fields directly to core `RangeBar`

**Decision:** **B - Wrapper pattern with `DukascopyRangeBar`**

**Implications:**
```rust
pub struct DukascopyRangeBar {
    pub base: RangeBar,           // Standard OHLCV + microstructure
    pub spread_stats: SpreadStats, // Dukascopy-specific metrics
}

pub struct SpreadStats {
    pub avg_spread: FixedPoint,
    pub min_spread: FixedPoint,
    pub max_spread: FixedPoint,
    pub tick_count: u32,
    pub total_bid_liquidity: FixedPoint,
    pub total_ask_liquidity: FixedPoint,
}
```

**Rationale:**
- Zero changes to core `RangeBar` struct (maintains "zero core changes" principle)
- Preserves all Dukascopy-specific information
- Consumers can unwrap `.base` for standard bar access
- Clean separation: aggTrades → RangeBar, Dukascopy → DukascopyRangeBar

---

## Summary of All Decisions

| # | Aspect | Decision | Status |
|---|--------|----------|--------|
| Q1 | Volume semantics | Dual tracking (liquidity + tick_count) | ✅ |
| Q2 | Direction inference | `is_buyer_maker = None` (explicit unknown) | ✅ |
| Q3 | Structure | Wrapper pattern `DukascopyRangeBar { base, spread_stats }` | ✅ |
| Q4 | Incomplete bar handling | Implement `get_incomplete_bar()` in processor | ✅ |
| Q5 | Error handling | Module-level `DukascopyError` with `From` traits | ✅ |
| Q6 | Moving average | **REVISED:** SMA (per-bar averages, reset on close) | ✅ |
| Q7 | Zero-volume filtering | Process all ticks, track zero-volume frequency | ✅ |
| Q8 | Implementation sequencing | Parallel tracks with dependency gates | ✅ |
| Q9 | Documentation structure | Keep top-down flow + executive summary + quick reference | ✅ |
| Q10 | Buy/sell segregation | No segregation for Dukascopy (unknown direction) | ✅ |
| Q11 | RangeBar.volume value | `total_bid_liquidity + total_ask_liquidity` | ✅ |
| Q12 | Mid-price validation | Configurable strictness (Permissive/Strict/Paranoid) | ✅ |
| Q13 | SpreadStats reset | Reset all state when bar closes (per-bar semantics) | ✅ |
| Q14 | Zero-volume quantity | Use actual volume (0.0) from tick | ✅ |
| Q15 | Instrument config | Embed TOML in binary (include_str! macro) | ✅ |
| Q16 | Timestamp validation | Expand range to 2000-2035 | ✅ |
| Q17 | Timezone handling | No special handling (GMT = UTC) | ✅ |
| Q18 | Decimal factor validation | Type-specific price ranges (Forex: 0.01-10k) | ✅ |
| Q19 | Processor state | Add `current_bar_state` field (enables streaming) | ✅ |
| Q20 | Instrument type lookup | Infer from config path structure | ✅ |
| Q21 | SMA precision | Integer division correct for FixedPoint | ✅ |
| Q22 | Error recovery | Type-specific (Fatal vs Skip with 10% threshold) | ✅ |

---

## Next Steps
- ✅ **All 22 questions resolved** (Q1-Q22, including architectural deep-dive)
- ✅ **Zero critical blockers remaining** (all gaps and resolutions complete)
- ✅ **Design finalized** (implementation-ready with all edge cases covered)
- **Ready**: Begin implementation following updated checklist

---

**Last Updated:** 2025-10-02 (Q1-Q22 complete - implementation-ready)

### Q4: Incomplete Bar Handling ✅ DECIDED

**Question:** Where should partial bar state live?

**Context Discovery:**
- `RangeBarProcessor.get_incomplete_bar()` already exists (stubbed, returns `None`)
- `StreamingProcessor` already tries to use it for final bar at stream end
- Existing design intent: expose incomplete bars, just not yet implemented

**Options Considered:**
- A) Builder maintains shadow state (duplicate)
- B) ⭐ Implement `get_incomplete_bar()` in core processor
- C) No incomplete bar support

**Decision:** **B - Implement processor method (completes existing design)**

**Implications:**
```rust
// In RangeBarProcessor - IMPLEMENT existing stub
impl RangeBarProcessor {
    pub fn get_incomplete_bar(&self) -> Option<RangeBar> {
        self.current_bar_state.as_ref().map(|state| state.bar.clone())
    }
}

// In DukascopyRangeBarBuilder - wrap it with spread stats
impl DukascopyRangeBarBuilder {
    pub fn get_incomplete_bar(&self) -> Option<DukascopyRangeBar> {
        self.processor.get_incomplete_bar().map(|base| {
            DukascopyRangeBar {
                base,
                spread_stats: self.current_spread_stats.clone(),
            }
        })
    }
}
```

**Rationale:**
- Single source of truth (processor owns bar state)
- Benefits BOTH aggTrades and Dukascopy streaming
- Completes existing feature (not adding new one)
- Minimal implementation: expose existing internal state
- Streaming already depends on this - currently broken (returns None)

---


### Q5: Error Handling - Unknown Instrument ✅ DECIDED

**Question:** What happens when instrument isn't in config (unknown decimal factor)?

**Deep Dive Analysis:**
- Existing system already uses `Result<_, ProcessingError>` pattern
- `process_single_trade()` returns `Result<Option<RangeBar>, ProcessingError>`
- Integration is natural and clean

**Options Considered:**
- A) Panic/abort
- B) Return Result::Err (requires integration analysis)
- C) Use default decimal factor (silent corruption)
- D) Log warning + default

**Decision:** **B - Return Result with Approach 3 (Module-level error)**

**Implications:**
```rust
// New module-level error type
pub enum DukascopyError {
    Conversion(ConversionError),
    Processing(ProcessingError),
    Network(reqwest::Error),        // Future: HTTP fetching
    Decompression(std::io::Error),  // Future: LZMA errors
}

// Conversion-specific errors
pub enum ConversionError {
    UnsupportedInstrument { instrument: String },
    InvalidDecimalFactor { instrument: String },
}

// Automatic conversion with From trait
impl From<ProcessingError> for DukascopyError {
    fn from(e: ProcessingError) -> Self {
        DukascopyError::Processing(e)
    }
}

impl From<ConversionError> for DukascopyError {
    fn from(e: ConversionError) -> Self {
        DukascopyError::Conversion(e)
    }
}

// Builder signature
impl DukascopyRangeBarBuilder {
    pub fn process_tick(&mut self, tick: &DukascopyTick) 
        -> Result<Option<DukascopyRangeBar>, DukascopyError> 
    {
        let synthetic_trade = tick_to_synthetic_trade(tick, instrument)?; // Auto-converts
        let base_bar = self.processor.process_single_trade(synthetic_trade)?; // Auto-converts
        Ok(base_bar.map(|base| DukascopyRangeBar { base, spread_stats }))
    }
}
```

**Rationale:**
- Graceful error propagation (production-safe)
- Clean module boundaries (Dukascopy owns its error semantics)
- Future-proof (encompasses network, decompression errors)
- Automatic `?` operator conversion via `From` trait
- Clear error messages for debugging
- Prevents silent data corruption from wrong decimal factors

**Why Not Approach 2 (extend ProcessingError)?**
- Couples Dukascopy concerns to core error type
- Doesn't accommodate network/decompression errors naturally
- Less modular architecture

---


### Q6: Moving Average Definition ✅ DECIDED → REVISED TO SMA

**Question:** Which moving average type for spread tracking in real-time streaming?

**Original Research:** Studied talipp library for EMA patterns

**User Clarification:** "EMA is for calculating the spread for each rangebar" → Per-bar average needed, not rolling average

**Key Realization:**
- Per-bar semantics: Each bar's avg_spread reflects only ticks within that bar
- Reset on bar close: Stats are independent per bar
- **SMA is sufficient**: No need for EMA complexity when resetting every bar

**Options Reconsidered:**
- A) ⭐ Simple Moving Average (perfect for per-bar metrics)
- B) EMA (overkill for per-bar, designed for continuous smoothing)
- C) Volume-weighted average (unnecessary complexity)

**Decision:** **A - Simple Moving Average (SMA) with O(1) incremental calculation**

**Implications:**
```rust
pub struct SpreadStats {
    // Accumulators (reset on bar close)
    spread_sum: FixedPoint,           // Sum of all spreads
    bid_liquidity_sum: FixedPoint,    // Sum of all bid volumes
    ask_liquidity_sum: FixedPoint,    // Sum of all ask volumes

    // Min/Max (reset on bar close)
    min_spread: FixedPoint,
    max_spread: FixedPoint,

    // Counters (reset on bar close)
    tick_count: u32,
    zero_volume_tick_count: u32,

    // Totals (reset on bar close)
    total_bid_liquidity: FixedPoint,
    total_ask_liquidity: FixedPoint,
}

impl SpreadStats {
    pub fn new() -> Self {
        Self {
            spread_sum: FixedPoint::ZERO,
            bid_liquidity_sum: FixedPoint::ZERO,
            ask_liquidity_sum: FixedPoint::ZERO,
            min_spread: FixedPoint::MAX,
            max_spread: FixedPoint::MIN,
            tick_count: 0,
            zero_volume_tick_count: 0,
            total_bid_liquidity: FixedPoint::ZERO,
            total_ask_liquidity: FixedPoint::ZERO,
        }
    }

    pub fn update(&mut self, tick: &DukascopyTick) {
        let spread = FixedPoint::from_f64(tick.ask - tick.bid);
        let bid_vol = FixedPoint::from_f64(tick.bid_volume);
        let ask_vol = FixedPoint::from_f64(tick.ask_volume);

        // Accumulate for averages (O(1))
        self.spread_sum += spread;
        self.bid_liquidity_sum += bid_vol;
        self.ask_liquidity_sum += ask_vol;

        // Track min/max
        self.min_spread = self.min_spread.min(spread);
        self.max_spread = self.max_spread.max(spread);

        // Count ticks
        self.tick_count += 1;
        if tick.bid_volume == 0.0 && tick.ask_volume == 0.0 {
            self.zero_volume_tick_count += 1;
        }

        // Track totals
        self.total_bid_liquidity += bid_vol;
        self.total_ask_liquidity += ask_vol;
    }

    // Calculate averages on demand (O(1))
    pub fn avg_spread(&self) -> FixedPoint {
        if self.tick_count > 0 {
            FixedPoint(self.spread_sum.0 / self.tick_count as i64)
        } else {
            FixedPoint::ZERO
        }
    }

    pub fn avg_bid_liquidity(&self) -> FixedPoint {
        if self.tick_count > 0 {
            FixedPoint(self.bid_liquidity_sum.0 / self.tick_count as i64)
        } else {
            FixedPoint::ZERO
        }
    }

    pub fn avg_ask_liquidity(&self) -> FixedPoint {
        if self.tick_count > 0 {
            FixedPoint(self.ask_liquidity_sum.0 / self.tick_count as i64)
        } else {
            FixedPoint::ZERO
        }
    }
}
```

**Rationale:**
- Per-bar semantics: Averages calculated from ticks within each bar only
- O(1) updates: Just accumulate sums, divide at end
- O(1) queries: Division is computed on demand
- Minimal state: No EMATracker, no Option<T>, no period tracking
- Simple and correct: SMA is mathematically exact for per-bar averages
- Reset on bar close: Fresh SpreadStats for each new bar

**User Memory Update Removed:**
- No longer need talipp pattern reference (using SMA instead)

---


### Q7: Zero-Volume Tick Handling ✅ DECIDED

**Question:** Should Dukascopy zero-volume ticks be filtered or processed?

**Original Assumption:** Zero-volume ticks should be filtered (similar to Binance aggTrades)

**User Insight Challenge:** "Why do we need to filter out zero volume data? I don't think it matters because even when it's zero volume, as long as we have new tick coming in, then... perhaps we should be paying a lot of attention to it to see if we ever encounter such data from the data stream."

**Key Realization:**
- Zero-volume ticks still represent market activity (quote updates)
- Tick arrival itself is informationally significant
- Quote updates without volume indicate market maker repositioning
- Filtering discards potentially valuable market microstructure data

**Decision:** **Process all ticks regardless of volume (no filtering)**

**Implications:**
```rust
// In DukascopyRangeBarBuilder - no volume filtering
impl DukascopyRangeBarBuilder {
    pub fn process_tick(&mut self, tick: &DukascopyTick)
        -> Result<Option<DukascopyRangeBar>, DukascopyError>
    {
        // Process ALL ticks - no volume check
        let synthetic_trade = tick_to_synthetic_trade(tick, instrument)?;
        let base_bar = self.processor.process_single_trade(synthetic_trade)?;

        // Track zero-volume ticks as metric
        if tick.bid_volume == 0.0 && tick.ask_volume == 0.0 {
            self.spread_stats.zero_volume_tick_count += 1;
        }

        Ok(base_bar.map(|base| DukascopyRangeBar { base, spread_stats }))
    }
}

// Enhanced SpreadStats to track zero-volume frequency
pub struct SpreadStats {
    pub avg_spread: EMATracker,
    pub avg_bid_liquidity: EMATracker,
    pub avg_ask_liquidity: EMATracker,
    pub min_spread: FixedPoint,
    pub max_spread: FixedPoint,
    pub tick_count: u32,
    pub zero_volume_tick_count: u32,  // NEW: track frequency
    pub total_bid_liquidity: FixedPoint,
    pub total_ask_liquidity: FixedPoint,
}
```

**Rationale:**
- Tick arrival = market activity, regardless of volume
- Zero-volume ticks reveal market maker quote adjustments
- Filtering loses microstructure information
- Tracking frequency provides valuable diagnostic metric
- Aligns with Forex market reality (quotes update frequently)

**Semantic Difference from aggTrades:**
- aggTrades = actual executed trades (always have volume by definition)
- Dukascopy ticks = market maker quotes (volume optional)
- Zero-volume tick = quote repositioning without matched trade

---


### Q8: Implementation Sequencing ✅ DECIDED

**Question:** Should implementation follow strict phased approach or allow parallel work?

**Context:**
- Original design proposed Phase 1 (core adapter) → validate → Phase 2 (enhancements)
- All core decisions now made (Q1-Q7)
- Dependencies clearly identified

**Options Considered:**
- A) Strict sequential: Phase 1 complete → validate → Phase 2 start
- B) Parallel work with dependency tracking (core + tests simultaneously)
- C) Flexible: implement features as dependencies resolve

**Decision:** **B - Parallel work with dependency tracking**

**Implications:**

**Can Start Immediately (Parallel Track 1 - Core):**
```rust
// 1. DukascopyTick struct + binary parser
// 2. InstrumentConfig + decimal factor lookup
// 3. tick_to_synthetic_trade() conversion
// 4. DukascopyError enum + From traits
// 5. Implement RangeBarProcessor::get_incomplete_bar()
```

**Can Start Immediately (Parallel Track 2 - Support):**
```rust
// 1. EMATracker struct (reusable helper)
// 2. SpreadStats struct definition
// 3. DukascopyRangeBar wrapper struct
// 4. Unit tests for EMATracker
```

**Sequential Dependencies:**
```
DukascopyRangeBarBuilder depends on:
  ✓ tick_to_synthetic_trade()  (Track 1)
  ✓ DukascopyError             (Track 1)
  ✓ SpreadStats                (Track 2)
  ✓ DukascopyRangeBar          (Track 2)
```

**Rationale:**
- Core and support structures are independent
- Tests can be written alongside implementation
- Faster delivery (parallel work streams)
- Clear dependency gates prevent integration issues
- Both tracks converge at builder implementation

**Implementation Order:**
1. **Week 1**: Core conversion logic + EMATracker + tests
2. **Week 2**: Builder integration + SpreadStats tracking
3. **Week 3**: Validation suite + documentation
4. **Week 4**: Integration tests + performance benchmarks

---


### Q9: Documentation Structure ✅ DECIDED

**Question:** Should design document maintain current flow or reorganize for different audiences?

**Current Structure:**
1. Problem Statement (data structure mismatch)
2. Adapter Pattern (architectural approach)
3. Mid-Price Conversion (technical details)
4. Volume Semantics (dual tracking)
5. Implementation Details (code structure)
6. Validation Strategy (testing)

**Options Considered:**
- A) Keep current top-down flow (problem → solution → implementation)
- B) Reorganize to Quick Start → Deep Dive → Advanced Topics
- C) Split into multiple documents (architecture, implementation, API)

**Decision:** **A - Keep current top-down flow (with enhancement)**

**Enhancement Applied:**
```markdown
# NEW: Executive Summary section at top
## Executive Summary (2-3 paragraphs)
- Core challenge: Dukascopy provides quotes, not trades
- Solution: Adapter pattern with mid-price conversion
- Key insight: Preserve all Forex-specific information (spread stats)
- Implementation: Zero core changes, wrapper pattern

[Rest of document maintains current structure]

# NEW: Quick Reference section before Implementation Details
## Quick Reference
- Conversion: mid_price = (bid + ask) / 2
- Volume: dual tracking (liquidity + tick_count)
- Direction: is_buyer_maker = None
- Structure: DukascopyRangeBar { base, spread_stats }
- Errors: DukascopyError with From traits
```

**Rationale:**
- Current flow is logical and comprehensive
- Adding executive summary helps busy readers
- Quick reference provides implementation cheat sheet
- Avoids complexity of multi-document maintenance
- Single source of truth for design decisions

**Document Sections (Final):**
1. Executive Summary (NEW)
2. Problem Statement
3. Architectural Approach (Adapter Pattern)
4. Technical Details (mid-price, volume, direction)
5. Quick Reference (NEW)
6. Implementation Structure
7. Validation Strategy
8. Phase Planning

---


### Q10: Buy/Sell Segregation for Dukascopy ✅ DECIDED

**Question:** How to handle buy/sell volume segregation when trade direction is unknown?

**Context Discovery:**
- RangeBar tracks `buy_volume`/`sell_volume` separately for order flow analysis
- Segregation requires `is_buyer_maker` boolean to classify trades
- Q2 decided: Dukascopy should use `is_buyer_maker = None` (explicit unknown)
- But AggTrade uses `bool` (not `Option<bool>`) throughout codebase

**Gap Analysis:**
- Changing to `Option<bool>` = BREAKING CHANGE to core struct
- Affects ~200 lines across types.rs (new, update_with_trade, tests)
- Buy/sell segregation logic requires boolean decision

**Options Considered:**
- A) Change AggTrade to `is_buyer_maker: Option<bool>` (breaking change)
- B) Use `is_buyer_maker = false` as default (lose explicit unknown semantics)
- C) Add `unknown_direction_volume` field to RangeBar (new field)
- D) ⭐ Don't track buy/sell segregation for Dukascopy at all

**Decision:** **D - No buy/sell segregation for Dukascopy data**

**Implications:**
```rust
// In tick_to_synthetic_trade()
AggTrade {
    // ... other fields
    is_buyer_maker: false,  // Arbitrary default (not used)
    // Volume goes to RangeBar.volume, but buy_volume/sell_volume stay at 0
}

// In DukascopyRangeBar
pub struct DukascopyRangeBar {
    pub base: RangeBar {
        volume: FixedPoint,        // Total (bid_vol + ask_vol)
        buy_volume: FixedPoint(0), // Not tracked (unknown direction)
        sell_volume: FixedPoint(0), // Not tracked (unknown direction)
        // ... other fields
    },
    pub spread_stats: SpreadStats {
        // Dukascopy-specific metrics capture what we CAN measure:
        total_bid_liquidity: FixedPoint,  // Available at bid
        total_ask_liquidity: FixedPoint,  // Available at ask
        // ... other fields
    }
}
```

**Rationale:**
- Semantic honesty: Don't pretend to know trade direction when we don't
- aggTrades = real trades → full microstructure (buy/sell segregation)
- Dukascopy = quotes → no trade direction → no segregation
- SpreadStats captures what we CAN measure (bid/ask liquidity asymmetry)
- Zero breaking changes to core AggTrade struct
- Clean separation of data source semantics

**What Goes in RangeBar.volume?**
- For Dukascopy: `volume = total_bid_liquidity + total_ask_liquidity`
- buy_volume and sell_volume both stay at FixedPoint(0)
- Downstream consumers see volume (for activity tracking) but no directional flow

**Trade Count Handling:**
- `individual_trade_count` = tick_count (each quote = 1 "trade")
- `buy_trade_count` = 0 (unknown direction)
- `sell_trade_count` = 0 (unknown direction)

**Note:** This decision resolves GAP-1 and GAP-2 from integration analysis.

---


### Q11: RangeBar.volume Field Value ✅ DECIDED (Implicit from Q10)

**Question:** With dual tracking (Q1), what single value goes into RangeBar.volume?

**Resolution:** Q10 decision answers this:
- `RangeBar.volume = total_bid_liquidity + total_ask_liquidity`
- Represents total available liquidity aggregated across all ticks in the bar
- Semantic meaning: "How much liquidity was offered during this price movement"

**Implications:**
```rust
// In tick_to_synthetic_trade()
let total_liquidity = tick.ask_volume + tick.bid_volume;

AggTrade {
    volume: FixedPoint::from_f64(total_liquidity),
    // This flows through to RangeBar.volume
}

// RangeBar accumulates this across all ticks
self.volume += trade.volume;  // Sum of all liquidity seen
```

**Rationale:**
- Single value for compatibility with existing RangeBar API
- SpreadStats tracks granular metrics (bid vs ask separately)
- Consumers can use `.base.volume` for total or `.spread_stats.total_bid_liquidity` for specifics

**Note:** This decision resolves GAP-2 from integration analysis.

---


### Q12: Mid-Price Edge Case Validation ✅ DECIDED

**Question:** How to handle edge cases in mid-price calculation?

**Edge Cases Identified:**
1. `bid = 0.0` → mid_price = ask/2 (WRONG)
2. `ask = 0.0` → mid_price = bid/2 (WRONG)
3. `bid > ask` → crossed market (data corruption)
4. `spread > 10%` → stale quote (outlier)

**Real-World Occurrence:**
- Market open: First tick may have bid=0 or ask=0
- Network issues: Stale quotes with inverted bid/ask
- Flash crashes: Temporary crossed markets

**Options Considered:**
- A) No validation (trust Dukascopy data quality)
- B) Validate and skip invalid ticks
- C) Validate and return Result::Err
- D) ⭐ Validate with configurable strictness

**Decision:** **D - Validate with configurable strictness levels**

**Implications:**
```rust
pub enum ValidationStrictness {
    Permissive,  // Warn but process (log outliers)
    Strict,      // Error on invalid ticks
    Paranoid,    // Error on suspicious patterns (spread > 1%, etc.)
}

pub fn validate_tick(
    tick: &DukascopyTick,
    strictness: ValidationStrictness,
) -> Result<(), ConversionError> {
    // Critical checks (all levels)
    if tick.bid <= 0.0 {
        return Err(ConversionError::InvalidBid { bid: tick.bid });
    }
    if tick.ask <= 0.0 {
        return Err(ConversionError::InvalidAsk { ask: tick.ask });
    }
    if tick.bid > tick.ask {
        return Err(ConversionError::CrossedMarket {
            bid: tick.bid,
            ask: tick.ask
        });
    }

    // Strictness-dependent checks
    match strictness {
        ValidationStrictness::Permissive => Ok(()),
        ValidationStrictness::Strict => {
            let spread_pct = (tick.ask - tick.bid) / tick.bid * 100.0;
            if spread_pct > 10.0 {
                return Err(ConversionError::ExcessiveSpread {
                    spread_pct
                });
            }
            Ok(())
        }
        ValidationStrictness::Paranoid => {
            let spread_pct = (tick.ask - tick.bid) / tick.bid * 100.0;
            if spread_pct > 1.0 {
                return Err(ConversionError::SuspiciousSpread {
                    spread_pct
                });
            }
            // Additional checks: reasonable price ranges, etc.
            Ok(())
        }
    }
}

// In DukascopyRangeBarBuilder
pub struct DukascopyRangeBarBuilder {
    processor: RangeBarProcessor,
    tick_counter: i64,
    validation_strictness: ValidationStrictness,  // Configurable
}

impl DukascopyRangeBarBuilder {
    pub fn process_tick(&mut self, tick: &DukascopyTick)
        -> Result<Option<DukascopyRangeBar>, DukascopyError>
    {
        // Validate first
        validate_tick(tick, self.validation_strictness)?;

        // Calculate mid-price (guaranteed valid now)
        let mid_price = (tick.ask + tick.bid) / 2.0;

        // Continue processing...
    }
}
```

**Enhanced ConversionError:**
```rust
pub enum ConversionError {
    UnsupportedInstrument { instrument: String },
    InvalidDecimalFactor { instrument: String },
    InvalidBid { bid: f64 },
    InvalidAsk { ask: f64 },
    CrossedMarket { bid: f64, ask: f64 },
    ExcessiveSpread { spread_pct: f64 },
    SuspiciousSpread { spread_pct: f64 },
}
```

**Rationale:**
- Permissive: Production use (trust Dukascopy quality)
- Strict: Development/testing (catch obvious issues)
- Paranoid: Data quality audits (flag all anomalies)
- Configurable at builder construction time
- Clear error messages for debugging

**Default Recommendation:** `ValidationStrictness::Strict` for production

**Note:** This decision resolves GAP-3 from integration analysis.

---


### Q13: SpreadStats Reset Strategy ✅ DECIDED (Simplified with SMA)

**Question:** When range bar closes and new bar starts, what happens to SpreadStats?

**Context:**
- Q6 revised to use SMA (not EMA)
- Per-bar semantics: Each bar's statistics are independent
- Reset is now straightforward

**Decision:** **Reset all SpreadStats when bar closes**

**Implications:**
```rust
impl DukascopyRangeBarBuilder {
    pub fn process_tick(&mut self, tick: &DukascopyTick)
        -> Result<Option<DukascopyRangeBar>, DukascopyError>
    {
        // Validate tick
        validate_tick(tick, self.validation_strictness)?;

        // Update spread stats (accumulates within current bar)
        self.current_spread_stats.update(tick);

        // Convert to synthetic trade
        let synthetic_trade = tick_to_synthetic_trade(tick, self.tick_counter)?;
        self.tick_counter += 1;

        // Process through core processor
        let maybe_bar = self.processor.process_single_trade(synthetic_trade)?;

        // If bar closed, wrap it with spread stats
        if let Some(base) = maybe_bar {
            let completed_bar = DukascopyRangeBar {
                base,
                spread_stats: self.current_spread_stats.clone(), // Snapshot
            };

            // Reset for next bar (fresh state)
            self.current_spread_stats = SpreadStats::new();

            Ok(Some(completed_bar))
        } else {
            Ok(None)
        }
    }
}
```

**Rationale:**
- Per-bar semantics: Each bar's avg_spread reflects only its ticks
- Snapshot on close: Completed bar gets frozen SpreadStats
- Fresh start: New bar starts with zeroed accumulators
- Simple and clear: No state carryover between bars

**SpreadStats Lifecycle:**
1. Bar starts: `SpreadStats::new()` (all zeros)
2. Ticks arrive: `update()` accumulates sums
3. Bar closes: Clone current stats into DukascopyRangeBar
4. Reset: `SpreadStats::new()` for next bar

**Note:** This decision resolves GAP-4 from integration analysis.

---


### Q14: Zero-Volume Tick Quantity ✅ DECIDED

**Question:** When both bid_volume=0 and ask_volume=0, what quantity goes into synthetic AggTrade?

**Context:**
- Q7 decided: Process all ticks (including zero-volume)
- Zero-volume ticks still update price but have no liquidity

**Options Considered:**
- A) ⭐ Use volume = 0.0 as-is (actual value)
- B) Use volume = 1.0 (count as activity)
- C) Use volume = epsilon (minimal non-zero)

**Decision:** **A - Use actual volume (0.0) from the tick**

**Implications:**
```rust
// In tick_to_synthetic_trade()
let total_liquidity = tick.ask_volume + tick.bid_volume;  // Could be 0.0

AggTrade {
    volume: FixedPoint::from_f64(total_liquidity),  // Use actual value
    // No special handling needed
}

// In RangeBar processing
// Zero-volume ticks still update price (OHLC)
// But contribute 0 to volume accumulation
```

**Rationale:**
- Semantic honesty: If Dukascopy reports no volume, don't invent volume
- Price updates: Zero-volume ticks still move OHLC (mid-price changes)
- Volume tracking: RangeBar.volume reflects actual liquidity available
- Separate metric: SpreadStats.zero_volume_tick_count tracks frequency
- Safe: Existing codebase handles zero volume correctly (no division errors)

**Zero-Volume Tick Behavior:**
- Updates bar's open/high/low/close (price movement)
- Adds 0 to bar's volume (no liquidity contribution)
- Increments tick_count in SpreadStats
- Increments zero_volume_tick_count in SpreadStats
- Spreads still tracked (bid-ask difference exists even with no volume)

**Note:** This decision resolves GAP-5 from integration analysis.

---


### Q15: Instrument Config Source ✅ DECIDED

**Question:** How to load instrument configuration (decimal factors) at runtime?

**Context:**
- Config file exists: `docs/planning/research/dukascopy-instrument-config.toml`
- Contains 1,607 instruments with decimal factors
- Needed for price conversion (Dukascopy int → FixedPoint)

**Options Considered:**
- A) ⭐ Embed TOML in binary (include_str! macro)
- B) Load from file path at runtime
- C) Builder parameter (caller provides)
- D) Hybrid (embedded default + optional override)

**Decision:** **A - Embed TOML in compiled binary**

**Implications:**
```rust
// In src/data/dukascopy/config.rs
const INSTRUMENT_CONFIG_TOML: &str = include_str!(
    "../../../docs/planning/research/dukascopy-instrument-config.toml"
);

lazy_static! {
    static ref INSTRUMENT_CONFIG: InstrumentConfig = {
        toml::from_str(INSTRUMENT_CONFIG_TOML)
            .expect("Failed to parse embedded instrument config")
    };
}

pub fn get_decimal_factor(instrument: &str) -> Result<u32, ConversionError> {
    INSTRUMENT_CONFIG
        .instruments
        .get(instrument)
        .map(|i| i.decimal_factor)
        .ok_or_else(|| ConversionError::UnsupportedInstrument {
            instrument: instrument.to_string(),
        })
}

// Usage in tick_to_synthetic_trade()
fn tick_to_synthetic_trade(
    tick: &DukascopyTick,
    instrument: &str,
    id: i64,
) -> Result<AggTrade, ConversionError> {
    let decimal_factor = get_decimal_factor(instrument)?;

    // Convert with decimal factor
    let mid_price = (tick.ask + tick.bid) / 2.0;
    let price_fixed = FixedPoint::from_f64(mid_price * decimal_factor as f64);

    // ... rest of conversion
}
```

**Rationale:**
- Zero runtime dependencies: No file I/O needed
- Simpler deployment: Binary is self-contained
- Parse once: Config loaded at startup (lazy_static)
- Type safety: Compile-time guarantee config exists
- Fast lookups: HashMap in memory

**Trade-offs:**
- Must recompile to add new instruments (acceptable - rare event)
- Slightly larger binary size (~50KB for 1,607 instruments)
- Config updates require rebuild (acceptable for production)

**Note:** This decision resolves GAP-6 from integration analysis.

---


### Q16: Timestamp Validation Range ✅ DECIDED

**Question:** Current timestamp validator rejects pre-2020 data, but Dukascopy Forex starts from 2003. Expand range?

**Context:**
- Current range: 2020-2030 (for crypto data)
- Dukascopy Forex: Historical data from 2003+
- Validation in `src/core/timestamp.rs:44-45`

**Current Implementation:**
```rust
const MIN_TIMESTAMP: i64 = 1_577_836_800_000_000; // 2020-01-01
const MAX_TIMESTAMP: i64 = 1_893_456_000_000_000; // 2030-01-01
```

**Impact:** All pre-2020 historical Forex data fails validation

**Options Considered:**
- A) ⭐ Expand range to 2000-2035
- B) Skip validation for Dukascopy
- C) Make validation configurable

**Decision:** **A - Expand timestamp range to 2000-2035**

**Implications:**
```rust
// In src/core/timestamp.rs
pub fn validate_timestamp(timestamp: i64) -> bool {
    // Expanded bounds: 2000-01-01 to 2035-01-01
    const MIN_TIMESTAMP: i64 = 946_684_800_000_000;  // 2000-01-01
    const MAX_TIMESTAMP: i64 = 2_051_222_400_000_000; // 2035-01-01

    (MIN_TIMESTAMP..=MAX_TIMESTAMP).contains(&timestamp)
}
```

**Rationale:**
- Covers all historical Forex data (2003+)
- Covers all crypto data (2009+)
- Still validates against truly bogus timestamps (1970, 2100)
- Simple single-line change
- No conditional logic per data source

**Validation Still Catches:**
- Year 1970 (Unix epoch) - common error
- Year 1900 or earlier - data corruption
- Year 2100+ - future dates (typos)
- Negative timestamps - bugs

**Note:** This decision resolves GAP-7 from integration analysis.

---


### Q17: Timezone Handling ✅ DECIDED

**Question:** Dukascopy timestamps are GMT. Need special handling for timezone/DST?

**Context:**
- Dukascopy timestamps are GMT (per their documentation)
- normalize_timestamp() handles ms → μs conversion
- GMT ≈ UTC (sub-second difference, negligible)

**Options Considered:**
- A) ⭐ No special handling (treat GMT as UTC)
- B) Add explicit timezone conversion with chrono crate

**Decision:** **A - No special timezone handling**

**Implications:**
```rust
// In tick_to_synthetic_trade()
AggTrade {
    // Dukascopy provides ms timestamp (GMT)
    timestamp: normalize_timestamp(tick.timestamp),  // ms → μs
    // No timezone conversion needed
}
```

**Rationale:**
- GMT and UTC difference < 1 second (negligible for financial tick data)
- GMT/UTC don't observe DST (no transition issues)
- normalize_timestamp() already handles ms → μs correctly
- No additional dependencies (no chrono needed)
- Consistent with Binance data (also uses UTC timestamps)

**Note on DST:**
- GMT (Greenwich Mean Time) = UTC (no DST)
- No special handling needed for DST transitions
- Timestamps are continuous and monotonic

**Note:** This decision resolves GAP-9 from integration analysis.

---


### Q18: Decimal Factor Validation ✅ DECIDED

**Question:** Should we validate that decimal factors produce reasonable prices?

**Context:**
- Embedded config could have typos (1000 vs 100000)
- Wrong decimal factor → 100x price error (silent corruption)
- Need sanity check after conversion

**User Insight:** "For Forex and for forex only must be much more narrow range for prices"

**Decision:** **Instrument-type specific price range validation**

**Implications:**
```rust
pub enum InstrumentType {
    Forex,
    Crypto,
    Commodity,
    Equity,
}

pub fn validate_converted_price(
    price: f64,
    instrument_type: InstrumentType,
) -> Result<(), ConversionError> {
    let (min, max) = match instrument_type {
        InstrumentType::Forex => (0.01, 10_000.0),  // Narrow: all major pairs fit
        InstrumentType::Crypto => (0.0001, 1_000_000.0),  // Wide: SHIB to BTC
        InstrumentType::Commodity => (0.01, 100_000.0),  // Medium range
        InstrumentType::Equity => (0.01, 100_000.0),  // Medium range
    };

    if price < min || price > max {
        return Err(ConversionError::InvalidPriceRange {
            price,
            instrument_type,
            expected_range: (min, max),
        });
    }

    Ok(())
}

// In tick_to_synthetic_trade()
fn tick_to_synthetic_trade(
    tick: &DukascopyTick,
    instrument: &str,
    id: i64,
) -> Result<AggTrade, ConversionError> {
    let decimal_factor = get_decimal_factor(instrument)?;
    let instrument_type = get_instrument_type(instrument)?;

    let mid_price = (tick.ask + tick.bid) / 2.0;
    let converted_price = mid_price * decimal_factor as f64;

    // Validate converted price is reasonable
    validate_converted_price(converted_price, instrument_type)?;

    let price_fixed = FixedPoint::from_f64(converted_price);
    // ... rest of conversion
}
```

**Forex Price Range Justification (0.01 to 10,000):**
- **Major pairs:** 0.5 to 2.0 (EURUSD ~1.08, GBPUSD ~1.26)
- **Yen pairs:** 100 to 160 (USDJPY ~150)
- **Exotic pairs:** 0.01 to 10,000 covers all realistic ranges
- **Catches errors:** 100x typo would put EURUSD at 108 or 0.0108 (still in range, but close to bounds for detection)

**Enhanced InstrumentConfig:**
```rust
[instruments.EURUSD]
decimal_factor = 100000
instrument_type = "Forex"

[instruments.BTCUSD]
decimal_factor = 10
instrument_type = "Crypto"
```

**Rationale:**
- Forex: Tight validation (0.01-10,000) catches decimal factor typos
- Crypto: Loose validation (allows SHIB at $0.00001 and BTC at $100k)
- Type-aware: Each asset class has appropriate bounds
- Fail-fast: Bad config detected immediately, not after analysis

**Note:** This decision resolves GAP-10 from integration analysis.

---

## IMPLEMENTATION RESOLUTIONS (2025-10-02) ✅

**Critical architectural issues discovered during deep-dive audit, all resolved:**

### Q19: RangeBarProcessor State Management ✅ RESOLVED
**Problem:** Processor is stateless → `get_incomplete_bar()` stubbed (returns None)
**Resolution:** Add `current_bar_state: Option<RangeBarState>` field to RangeBarProcessor
**Impact:** Enables streaming use case, implements Q4 correctly, minimal breaking change

### Q20: InstrumentType Lookup ✅ RESOLVED
**Problem:** Config lacks explicit `instrument_type` field
**Resolution:** Infer from config path structure (`instruments.forex.*` → Forex)
**Impact:** Zero manual edits to 1,607 instruments, type-safe

### Q21: SMA Integer Division Precision ✅ VALIDATED
**Analysis:** Integer division on FixedPoint is mathematically correct
**Proof:** `(Σ value_i × 10^8) / N = (Σ value_i / N) × 10^8` (exact for FixedPoint)
**Impact:** No changes needed, current design optimal

### Q22: Error Recovery Policy ✅ RESOLVED
**Strategy:** Type-specific handling (Fatal vs Skip)
**Fatal:** Config errors (UnsupportedInstrument), Processing errors → Abort
**Skip:** Validation errors (CrossedMarket, ExcessiveSpread) → Log + Continue
**Safety:** Abort if >10% error rate (systemic data quality issue)

**Status:** All architectural decisions finalized. Implementation-ready.

---

